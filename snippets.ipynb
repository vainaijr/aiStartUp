{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snippets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "-_nVeZK9vZ8m",
        "6qj_CkF1gDUD",
        "75fczkzki9Fp",
        "ej3Uwo_smfTs",
        "I6_6BgQuzfuP",
        "Aw1ze6i3zpfR",
        "Wlndqa9tLrGB",
        "r3hEzo0EMcvz",
        "YXnd_IupPf5t",
        "y5Mal_jfXqjw",
        "G-VYs_PvZ5aj",
        "iPT2g2cVeucl",
        "os_xISCFfTU8",
        "2eubf-YzfcJo",
        "8kbPVTSzfesP",
        "jrZvY9Jugo4O",
        "K4EESxOmg0zv",
        "DJmW7gkJg8tj",
        "cXgyVNuXhBzF",
        "Jwwc34-a2F79",
        "NVqByHwi3aMZ",
        "-ovyMKbT81DU",
        "LrwUjNhx0Tfn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vainaijr/aiStartUp/blob/master/snippets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ep1ZiMgDZVb",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PldrprjiDeZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, unicode_literals, division\n",
        "from IPython.core.debugger import set_trace\n",
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torch.nn.utils.weight_norm import WeightNorm\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.optim import Adam\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomResizedCrop, ToTensor, Normalize\n",
        "from torchvision.transforms import CenterCrop, Resize, ColorJitter, ToPILImage\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset, sampler\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import copy, json, glob, time, math, os, datetime, string, random, re, warnings\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "from io import open\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.ion() # interactive mode\n",
        "\n",
        "# Install latest Tensorflow build\n",
        "# !pip install -q tf-nightly-2.0-preview\n",
        "!pip install tensorflow==2.0.0-alpha0 \n",
        "import tensorflow as tf\n",
        "from tensorflow import summary\n",
        "%load_ext tensorboard.notebook\n",
        "\n",
        "current_time = str(datetime.datetime.now().timestamp())\n",
        "train_log_dir = 'logs/tensorboard/train/' + current_time\n",
        "test_log_dir = 'logs/tensorboard/test/' + current_time\n",
        "train_summary_writer = summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_nVeZK9vZ8m",
        "colab_type": "text"
      },
      "source": [
        "# LeNet model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNqwG3hvcNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M9e4gt8wC6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nokBXc1CwQ7k",
        "colab_type": "text"
      },
      "source": [
        "# MNIST dataset, dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXZesVXdwVDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataset\n",
        "train_loader = DataLoader(datasets.MNIST('./', train=True, download=True,\n",
        "                                                         transform=Compose([\n",
        "                                                             ToTensor(),\n",
        "                                                             Normalize((0.1307,), (0.3081,))\n",
        "                                                         ])), batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# test dataset\n",
        "test_loader = DataLoader(datasets.MNIST('./', train=False, download=False,\n",
        "                                                         transform=Compose([\n",
        "                                                             ToTensor(),\n",
        "                                                             Normalize((0.1307,), (0.3081,))\n",
        "                                                         ])), batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "# GAN\n",
        "mnist_loader = DataLoader(datasets.MNIST('./', train=True, download=True,\n",
        "                                                         transform=Compose([\n",
        "                                                             Resize(img_size),\n",
        "                                                             ToTensor(),\n",
        "                                                             Normalize([0.5], [0.5])\n",
        "                                                         ])), batch_size=batch_size, shuffle=True,)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVFI3iDiw0hA",
        "colab_type": "text"
      },
      "source": [
        "# CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNlpQ5Z1w3dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHRC5Uqd_KGx",
        "colab_type": "text"
      },
      "source": [
        "# transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si7DDADJ_MLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.Resize(imsize),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_transform = Compose([\n",
        "    RandomCrop(200),\n",
        "    RandomHorizontalFlip(),\n",
        "    ColorJitter(),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "data_transforms = {\n",
        "    \n",
        "  'train' : Compose([\n",
        "    RandomResizedCrop(input_size),\n",
        "    RandomHorizontalFlip(),\n",
        "    ColorJitter(),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "  ]),\n",
        "  'val' : Compose([\n",
        "      Resize(input_size),\n",
        "      CenterCrop(input_size),\n",
        "      ToTensor(),\n",
        "      Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "  ])    \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qj_CkF1gDUD",
        "colab_type": "text"
      },
      "source": [
        "# unicodeToAscii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukG38gzUgKJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn a Unicode string to plain ASCII\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "      and c in all_letters\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75fczkzki9Fp",
        "colab_type": "text"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zi_uWuOi-Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    combined = torch.cat((input, hidden), 1)\n",
        "    hidden = self.i2h(combined)\n",
        "    output = self.i2o(combined)\n",
        "    output = self.softmax(output)\n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej3Uwo_smfTs",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuobQF5Gmhas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6_6BgQuzfuP",
        "colab_type": "text"
      },
      "source": [
        "# Lang"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIxw044TzgvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "    self.n_words = 2 # count SOS and EOS\n",
        "    \n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "  \n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw1ze6i3zpfR",
        "colab_type": "text"
      },
      "source": [
        "# normalizeString"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZso-1g7zr0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-z.!?]\", r\" \", s)\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlndqa9tLrGB",
        "colab_type": "text"
      },
      "source": [
        "# EncoderRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y4xjASwLsym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence.\n",
        "# for every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next\n",
        "# input word.\n",
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    output = embedded\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3hEzo0EMcvz",
        "colab_type": "text"
      },
      "source": [
        "# DecoderRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lia56vWzMesF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "  def forward(self, input, hidden):\n",
        "    output = self.embedding(input).view(1, 1, -1)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.softmax(self.out(output[0]))\n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXnd_IupPf5t",
        "colab_type": "text"
      },
      "source": [
        "# AttnDecoderRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyMbWoVLPhxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "    super(AttnDecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.dropout_p = dropout_p\n",
        "    self.max_length = max_length\n",
        "    \n",
        "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    self.dropout = nn.Dropout(self.dropout_p)\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "    self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "  \n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    embedded = self.dropout(embedded)\n",
        "    \n",
        "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "    \n",
        "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "    output = self.attn_combine(output).unsqueeze(0)\n",
        "    \n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    \n",
        "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "    return output, hidden, attn_weights\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Mal_jfXqjw",
        "colab_type": "text"
      },
      "source": [
        "# timeSince"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFS1uBxBXvLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def asMinutes(s):\n",
        "  m = math.floor(s / 60)\n",
        "  s -= m * 60\n",
        "  return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "  now = time.time()\n",
        "  s = now - since\n",
        "  es = s / (percent)\n",
        "  rs = es - s\n",
        "  return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-VYs_PvZ5aj",
        "colab_type": "text"
      },
      "source": [
        "# plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rdlMm4jZ73t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.switch_backend('agg')\n",
        "\n",
        "def showPlot(points):\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "  loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regulate intervals\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  plt.plot(points)\n",
        "  \n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Test dataset 'Horses'\")\n",
        "plt.imshow(test_A[0])\n",
        "plt.subplot(122)\n",
        "plt.title(\"Test dataset 'Zebras'\")\n",
        "plt.imshow(test_B[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPT2g2cVeucl",
        "colab_type": "text"
      },
      "source": [
        "# BiRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D-gWuc4ev-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recurrent Neural Networks (many to one)\n",
        "class BiRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "    super(BiRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "    # torch.nn.LSTM(*args, **kwargs)\n",
        "    # applies a multi layer long shot term memory RNN to an input sequence\n",
        "    \n",
        "    # parameters\n",
        "    # input_size - the number of expected features in the input x\n",
        "    # hidden_size - the number of features in the hidden state h\n",
        "    # num_layers - number of recurrent layers, example, setting num_layers = 2 would mean stacking two LSTMs together\n",
        "    # to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results\n",
        "    # default: 1\n",
        "    # bias - if False, then the layer does not use bias weights, default: True\n",
        "    # batch_first - if True, then the input and output tensors are provided as (batch, seq, feature), default: False\n",
        "    # dropout - if non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer,\n",
        "    # with dropout probability equal to dropout, default: 0\n",
        "    # bidirectional - if True, becomes a bidirectional LSTM, default: False\n",
        "    \n",
        "    # inputs - input, (h_0, c_0)\n",
        "    \n",
        "    # input of shape (seq_len, batch, input_size) - tensor containing the features of the input sequence, the input \n",
        "    # can also be a packed variable length sequence\n",
        "    \n",
        "    # h_0 of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the initial hidden state for \n",
        "    # each element in the batch, if the LSTM is bidirectional, num_directions should be 2, else it should be 1\n",
        "    \n",
        "    # c_0 of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the initial cell state for each\n",
        "    # element in the batch\n",
        "    \n",
        "    # if (h_0, c_0) is not provided, both h_0 and c_0 default to zero\n",
        "    \n",
        "    # outputs - output, (h_n, c_n)\n",
        "    \n",
        "    # output of shape (seq_len, batch, num_directions*hidden_size) - tensor containing the ouput features (h_t) from\n",
        "    # the last layer of the LSTM, for each t\n",
        "    \n",
        "    # h_n of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the hidden state for \n",
        "    # t = seq_len\n",
        "    \n",
        "    # c_n of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the cell state for t = seq_len\n",
        "    \n",
        "    self.fc = nn.Linear(hidden_size*2, num_classes)\n",
        "    # torch.nn.Linear(in_features, out_features, bias=True)\n",
        "    # applies a linear transformation to the incoming data\n",
        "\n",
        "    # parameters\n",
        "    # in_feature - size of each input sample\n",
        "    # out_feature - size of each output sample\n",
        "    # bias - if set to False, the layer will not learn an additive bias, default: True\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # set initial hidden and cell states\n",
        "    h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
        "    # torch.zeros(*sizes, out=None, layout=torch.strided, device=None, requires_grad=False)\n",
        "    # returns a tensor filled with the scalar value 0, with the shape defined by the variable argument sizes\n",
        "\n",
        "    # parameters\n",
        "    # sizes - a sequence of integers defining the shape of the output tensor, can be a variable number of argumentss\n",
        "    # or a collection like a list or tuple\n",
        "    # out - the output tensor\n",
        "    # dtype - the desired data type of returned tensor, default: if None, uses a global default\n",
        "    # layout - the desired layout of returned tensor, default: torch.strided\n",
        "    # device - the desired device of returned tensor\n",
        "    # requires_grad - if autograd should record operations on the returned tensor, default: False\n",
        "\n",
        "    c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
        "\n",
        "    # forward propagate LSTM\n",
        "    out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "    # outputs - output, (h_n, c_n)\n",
        "\n",
        "    # output of shape (seq_len, batch, num_directions*hidden_size) - tensor containing the ouput features (h_t) from\n",
        "    # the last layer of the LSTM, for each t\n",
        "\n",
        "    # h_n of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the hidden state for \n",
        "    # t = seq_len\n",
        "\n",
        "    # c_n of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the cell state for \n",
        "    # t = seq_len\n",
        "\n",
        "    # decode the hidden state of the last time step\n",
        "    out = self.fc(out[:, -1, :])\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os_xISCFfTU8",
        "colab_type": "text"
      },
      "source": [
        "# loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDevrOldfVQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # loss function, example, MSELoss, L1Loss, CTCLoss, NLLLoss, PoissonNLLLoss, KLDivLoss,BCELoss,\n",
        "# BCEWithLogitsLoss, MarginRankingLoss, HingeEmbeddingLoss, MultiLabelMarginLoss, SmoothL1Loss, SoftMarginLoss, \n",
        "# MultiLabelSoftMarginLoss, CosineEmbeddingLoss, MultiMarginLoss, TripletMarginLoss\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # optimizer, example, Adadelta, Adagrad, SparseAdam, Adamax,\n",
        "# ASGD, LBFGS, RMSprop, Rprop, SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eubf-YzfcJo",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5kwAGuDfdQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        # reshape(*shape) -> Tensor\n",
        "        # returns a tensor with the same data and number of elements as self but with the specified shape.\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # backward and optimize\n",
        "        optimizer.zero_grad() # set gradients of all model parameters to zero\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kbPVTSzfesP",
        "colab_type": "text"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNVQ5H_zffkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        \n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = model(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1) \n",
        "        \n",
        "        # torch.max(input, dim, keepdim=False, out=None) returns a namedtuple (values, indices) where values is the maximum\n",
        "        # value of each row of the input tensor in the given dimension dim, and indices is the index location of each maximum\n",
        "        # value found (argmax).\n",
        "        # If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of\n",
        "        # size 1\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        \n",
        "        correct += (predicted == labels).sum().item() \n",
        "        \n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100*correct / total))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5S9dsaBgaah",
        "colab_type": "text"
      },
      "source": [
        "# make_grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pqEXKI0gbw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(image):\n",
        "  if isinstance(image, torch.Tensor):\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "  else:\n",
        "    image = np.array(image).transpose((1, 2, 0))\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  image = std * image + mean\n",
        "  image = np.clip(image, 0, 1)\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
        "  plt.imshow(image)\n",
        "  ax.axis('off')\n",
        "\n",
        "images, _ = next(iter(train_loader))\n",
        "out = make_grid(images, nrow=8)\n",
        "imshow(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrZvY9Jugo4O",
        "colab_type": "text"
      },
      "source": [
        "# Ignite import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I9H854Igqjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignite is a high level library to help with training neural networks in PyTorch, it comes with an Engine to setup\n",
        "# a training loop, various metrics, handlers\n",
        "!pip install ignite\n",
        "from ignite.engine import Engine, Events \n",
        "# Engine - runs a given process_function over each batch of a dataset, emitting events as it goes\n",
        "# Events - allows users to attach functions to an Engine to fire functions at a specific event, example: \n",
        "# EPOCH_COMPLETED, ITERATION_STARTED etc.\n",
        "\n",
        "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
        "# Accuracy - metric to calculate accuracy over a dataset, for binary, multiclass, multilabel cases\n",
        "# Loss - general metric that takes a loss function as a parameter, calculate loss over a dataset\n",
        "# RunningAverage - general metric to attach to Engine during training\n",
        "\n",
        "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
        "# ModelCheckpoint - handler to checkpoint models\n",
        "# EarlyStopping - handler to stop training based on a score function\n",
        "\n",
        "from ignite.contrib.handlers import ProgressBar\n",
        "# ProgressBar - handler to create a tqdm progress bar, tqdm means progress in Arabic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EESxOmg0zv",
        "colab_type": "text"
      },
      "source": [
        "# TextCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK75h1Meg2Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TextCNN model\n",
        "# the model works for variable text lengths, we embed the words of a sentence, use convolutions, maxpooling\n",
        "# and concatenation to embed the sentence as a single vector\n",
        "# the single vector is passed through a fully connected layer with sigmoid to output a single value\n",
        "# this value can be interpreted as the probability a sentence is positive (closer to 1) or negative (closer to 0)\n",
        "\n",
        "# the minimum length of text expected by the model is the size of the smallest kernel size of the model\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, kernel_sizes, num_filters, num_classes, d_prob, mode):\n",
        "    super(TextCNN, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.kernel_sizes = kernel_sizes\n",
        "    self.num_filters = num_filters\n",
        "    self.num_classes = num_classes\n",
        "    self.d_prob = d_prob\n",
        "    self.mode = mode\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    self.load_embeddings()\n",
        "    self.conv = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k,\n",
        "                                        stride=1) for k in kernel_sizes])\n",
        "    self.dropout = nn.Dropout(d_prob)\n",
        "    self.fc = nn.Linear(len(kernel_sizes)*num_filters, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size, sequence_length = x.shape\n",
        "    x = self.embedding(x).transpose(1, 2)\n",
        "    x = [F.relu(conv(x)) for conv in self.conv]\n",
        "    x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]\n",
        "    x = torch.cat(x, dim=1)\n",
        "    x = self.fc(self.dropout(x))\n",
        "    return torch.sigmoid(x).squeeze()\n",
        "  \n",
        "  def load_embeddings(self):\n",
        "    if 'static' in self.mode:\n",
        "      self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
        "      if 'non' not in self.mode:\n",
        "        self.embedding.weight.data.requires_grad = False\n",
        "        print('Loaded pretrained embeddings, weights are not trainable')\n",
        "      else:\n",
        "        self.embedding.weight.data.requires_grad = True\n",
        "        print('Loaded pretrained embeddings, weights are trainable')\n",
        "      \n",
        "    elif self.mode == 'rand':\n",
        "      print('Randoml initialized embeddings are used')\n",
        "    else:\n",
        "      raise ValueError('Unexpected value of mode')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJmW7gkJg8tj",
        "colab_type": "text"
      },
      "source": [
        "# Ignite process_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr7oIZEUg-4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training and evaluating using Ignite\n",
        "# Ignite's Engine allows user to define a process_function a given batch, this is applied to all the batches of \n",
        "# the dataset, this is a general class that can be applied to train and validate models, a process_function\n",
        "# has two parameters, engine and batch\n",
        "\n",
        "# the function of the trainer\n",
        "# sets model in train mode\n",
        "# sets the gradient of the optimizer to zero\n",
        "# generate x and y from batch\n",
        "# performs a forward pass to calculate y_pred using model and x\n",
        "# calculates loss using y_pred and y\n",
        "# performs a backward pass using loss to calculate gradients for the model parameters\n",
        "# model parameters are optimized using gradients and optimizer\n",
        "# returns scalar loss\n",
        "\n",
        "def process_function(engine, batch):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  x, y = batch.text, batch.label\n",
        "  y_pred = model(x)\n",
        "  loss = criterion(y_pred, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXgyVNuXhBzF",
        "colab_type": "text"
      },
      "source": [
        "# Ignite eval_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8ZoZE-IhDhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluator engine - process_function\n",
        "\n",
        "# similar to the training process function, we setup a function to evaluate a single batch\n",
        "\n",
        "# eval_function\n",
        "# sets model in eval mode\n",
        "# generates x and y from batch\n",
        "# with torch.no_grad(), no gradients are calculated for any succeeding steps\n",
        "# performs a forward pass on the model to calculate y_pred based on model and x\n",
        "# returns y_pred and y\n",
        "\n",
        "# Ignite suggests attaching metrics to evaluators and not trainers\n",
        "\n",
        "# all metrics in Ignite require y_pred and y as outputs of the function attached to the Engine\n",
        "\n",
        "def eval_function(engine, batch):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    x, y = batch.text, batch.label\n",
        "    y_pred = model(x)\n",
        "    return y_pred, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwwc34-a2F79",
        "colab_type": "text"
      },
      "source": [
        "# CustomDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDfZeD4U2Iol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def __len__(self):\n",
        "    return 0\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return None\n",
        "  \n",
        "class FaceLandmarksDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    # csv_file (string): path to the csv file with annotations.\n",
        "    # root_dir (string): directory with all the images.\n",
        "    # transform (callable, optional): optional transform to be applied on a sample.\n",
        "    \n",
        "    self.landmarks_frame = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.landmarks_frame)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
        "    image = io.imread(img_name)\n",
        "    landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix()\n",
        "    landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "    sample = {'image': image, 'landmarks': landmarks}\n",
        "    \n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    \n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVqByHwi3aMZ",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo_nLRFV3cw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training helpers\n",
        "\n",
        "# list(get_trainable(model.parameters()))\n",
        "def get_trainable(model_params):\n",
        "  return (p for p in model_params if p.requires_grad)\n",
        "\n",
        "# list(get_frozen(model.parameters()))\n",
        "def get_frozen(model_params):\n",
        "  return (p for p in model_params if not p.requires_grad)\n",
        "\n",
        "# all_trainable(model.parameters())\n",
        "def all_trainable(model_params):\n",
        "  return all(p.requires_grad for p in model.params)\n",
        "\n",
        "# all_frozen(model.parameters())\n",
        "def all_frozen(model_params):\n",
        "  return all(not p.requires_grad for p in model.params)\n",
        "\n",
        "def freeze_all(model_params):\n",
        "  for param in model_params:\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ovyMKbT81DU",
        "colab_type": "text"
      },
      "source": [
        "# pretrainedmodels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfUGpT8V823d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pretrainedmodels\n",
        "import pretrainedmodels\n",
        "print(pretrainedmodels.model_names)\n",
        "\n",
        "model_name = 'nasnetalarge' # could be fbresnet152 or inceptionresnetv2\n",
        "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrwUjNhx0Tfn",
        "colab_type": "text"
      },
      "source": [
        "# train_model_helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QE_9BGv0XIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "  since = time.time()\n",
        "  \n",
        "  val_acc_history = []\n",
        "  \n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train() # set model to training mode\n",
        "      else:\n",
        "        model.eval() # set model to eval mode\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      # iterate over data\n",
        "      for inputs, labels in dataloader[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          # get model outputs and calculate loss\n",
        "          # special case for inception because in training it has an auxiliary output.\n",
        "          # in train mode we calculate the loss by summing the final output and the auxiliary output but in testing\n",
        "          # we only consider the final output.\n",
        "          if is_inception and phase == 'train':\n",
        "            outputs, aux_outputs = model(inputs)\n",
        "            loss1 = criterion(outputs, labels)\n",
        "            loss2 = criterion(aux_outputs, labels)\n",
        "            loss = loss1 + 0.4*loss2\n",
        "          else:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "\n",
        "          # backward + optimize only if in training phase\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    # deep copy the model\n",
        "    if phase == 'val' and epoch_acc > best_acc:\n",
        "      best_acc = epoch_acc\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if phase == 'val':\n",
        "      val_acc_history.append(epoch_acc)\n",
        "\n",
        "  print()\n",
        "  \n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapse // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "  \n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, val_acc_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4W5iU2RkhwT",
        "colab_type": "text"
      },
      "source": [
        "# datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmyZmuDkjzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train',\n",
        "                                                                                                   'val']}\n",
        "\n",
        "# create training and vaildation dataloaders\n",
        "dataloaders_dict = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in\n",
        "                  ['train', 'val']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHPmxCtHqjiV",
        "colab_type": "text"
      },
      "source": [
        "# plot_compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq6fax2mqkr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
        "plt.xlabel(\"Training epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.plot(range(1, num_epochs+1), ohist, label=\"Pretrained\")\n",
        "plt.plot(range(1, num_epochs+1), shist, label=\"Scratch\")\n",
        "plt.ylim((0,1.))\n",
        "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipyk8gJ3AQYE",
        "colab_type": "text"
      },
      "source": [
        "# STN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeyUfA7bAR1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "    \n",
        "    # Spatial transformer localization-network\n",
        "    self.localization = nn.Sequential(\n",
        "        nn.Conv2d(1, 8, kernel_size=7),\n",
        "        nn.MaxPool2d(2, stride=2),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(8, 10, kernel_size=5),\n",
        "        nn.MaxPool2d(2, stride=2),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    \n",
        "    # regressor for the 3 * 2 affine matrix\n",
        "    self.fc_loc = nn.Sequential(\n",
        "        nn.Linear(10 * 3 * 3, 32),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(32, 3 * 2)\n",
        "    )\n",
        "    \n",
        "    # initialize the weights/bias with identity transformation\n",
        "    self.fc_loc[2].weight.data.zero_()\n",
        "    self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "    \n",
        "  # spatial transformer network forward function\n",
        "  def stn(self, x):\n",
        "    xs = self.localization(x)\n",
        "    xs = xs.view(-1, 10 * 3 * 3)\n",
        "    theta = self.fc_loc(xs)\n",
        "    theta = theta.view(-1, 2, 3)\n",
        "    \n",
        "    grid = F.affine_grid(theta, x.size())\n",
        "    x = F.grid_sample(x, grid)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transform the input\n",
        "    x = self.stn(x)\n",
        "    \n",
        "    # perform forward pass\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "  \n",
        "model = Net().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfo9ULtNC9E4",
        "colab_type": "text"
      },
      "source": [
        "# train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIrTorbiC-lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  # total_step = len(train_loader)\n",
        "  for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "    # move tensors to the configured device\n",
        "    images = images.to(device)\n",
        "    # reshape(*shape) -> Tensor\n",
        "    # returns a tensor with the same data and number of elements as self but with the specified shape.\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    outputs = model(images)\n",
        "    loss = F.nll_loss(outputs, labels)\n",
        "\n",
        "    # backward and optimize\n",
        "    optimizer.zero_grad() # set gradients of all model parameters to zero\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx) % 500 == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(images), \n",
        "                                                                       len(train_loader.dataset), \n",
        "                                                                       100. * batch_idx / len(train_loader), \n",
        "                                                                       loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Q8TZDeDBlh",
        "colab_type": "text"
      },
      "source": [
        "# test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEdBjQeCDC3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  with torch.no_grad():\n",
        "      model.eval()\n",
        "      test_loss = 0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in test_loader:\n",
        "\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(images)\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1) \n",
        "\n",
        "          test_loss += F.nll_loss(outputs, labels, size_averag=False).item()\n",
        "\n",
        "          pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      \n",
        "      test_loss /= len(test_loader.dataset)\n",
        "      \n",
        "      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct,\n",
        "                                                                                  len(test_loader.dataset),\n",
        "                                                                                  100. * correct / \n",
        "                                                                                  len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eve2ggA_DnPk",
        "colab_type": "text"
      },
      "source": [
        "# tensor to numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es-6eB4bDo0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_image_np(inp):\n",
        "  inp = inp.numpy().transpose((1, 2, 0))\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  inp = std * inp + mean\n",
        "  inp = np.clip(inp, 0, 1)\n",
        "  return inp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr81DW_iE0mA",
        "colab_type": "text"
      },
      "source": [
        "# visualize stn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8yjrepRE19Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_stn():\n",
        "  with torch.no_grad():\n",
        "    # get a batch of training data\n",
        "    data = next(iter(test_loader))[0].to(device)\n",
        "    \n",
        "    input_tensor = data.cpu()\n",
        "    transformed_input_tensor = model.stn(data).cpu()\n",
        "    \n",
        "    in_grid = convert_image_np(make_grid(input_tensor))\n",
        "    \n",
        "    out_grid = convert_image_np(make_grid(transformed_input_tensor))\n",
        "    \n",
        "    # plot the results side by side\n",
        "    f, axarr = plt.subplots(1, 2)\n",
        "    axarr[0].imshow(in_grid)\n",
        "    axarr[0].set_title('Dataset Images')\n",
        "    \n",
        "    axarr[1].imshow(out_grid)\n",
        "    axarr[1].set_title('Transformed Images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwGErx7gQ2sp",
        "colab_type": "text"
      },
      "source": [
        "# image loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2nDpfRHQ4Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_loader(image_name):\n",
        "  image = Image.open(image_name)\n",
        "  # fake batch dimension required to fit network's input dimensions\n",
        "  image = transforms(image).unsqueeze(0)\n",
        "  return image.to(device, torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZdimh1GSA9L",
        "colab_type": "text"
      },
      "source": [
        "# tensor to PIL show"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkZ5pj_YSCp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unloader = ToPILImage() # reconvert into PIL image\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "  image = tensor.cpu().clone() # we clone the tensor to not do changes on it\n",
        "  image = image.squeeze(0)\n",
        "  image = unloader(image)\n",
        "  plt.imshow(image)\n",
        "  if title is not None:\n",
        "    plt.title(title)\n",
        "  plt.pause(0.001) # pause a bit so that plots are updated\n",
        "\n",
        "plt.figure()\n",
        "imshow(style_img, title='Style Image')\n",
        "\n",
        "plt.figure()\n",
        "imshow(content_img, title='Content Image')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "481sMJnMUdGk",
        "colab_type": "text"
      },
      "source": [
        "# gram matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omw6AIm-UfR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(input):\n",
        "  a, b, c, d = input.size() \n",
        "  # a = batch size(=1)\n",
        "  # b = number of feature maps\n",
        "  # (c, d) = dimensions of a f. map (N=c*d)\n",
        "  \n",
        "  features = input.view(a * b, c * d)\n",
        "  \n",
        "  G = torch.mm(features, features.t()) # compute the gram product \n",
        "  \n",
        "  # we 'normalize' the values of the gram matrix by dividing by the number of element in each feature maps.\n",
        "  return G.div(a * b * c * d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-53z9ER2cFT6",
        "colab_type": "text"
      },
      "source": [
        "# Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvOOT1gHcHWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a module to normalize input image so we can easily put it in a nn.Sequential\n",
        "class Normalization(nn.Module):\n",
        "  def __init__(self, mean, std):\n",
        "    super(Normalization, self).__init__()\n",
        "    # view the mean and std to make them [c x 1 x] so that they can directly work with image Tensor of shape\n",
        "    # [B x C x H x W].\n",
        "    # B is batch size.\n",
        "    # C is number of channels.\n",
        "    # H is height and W is width\n",
        "    self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "    self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "  \n",
        "  def forward(self, img):\n",
        "    # normalize img\n",
        "    return (img - self.mean) / self.std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u9Mqc11k3mC",
        "colab_type": "text"
      },
      "source": [
        "# FGSM attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zazsh7Dxk6Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FGSM attack code\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "  # collect the element-wise sign of the data gradient\n",
        "  sign_data_grad = data_grad.sign()\n",
        "  \n",
        "  # create the perturbed image by adjusting each pixel of the input image\n",
        "  perturbed_image = image + epsilon * sign_data_grad\n",
        "  \n",
        "  # adding clipping to maintain [0, 1] range\n",
        "  perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "  \n",
        "  return perturbed_image\n",
        "\n",
        "# fgsm attack test\n",
        "def test(model, device, test_loader, epsilon):\n",
        "  \n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  adv_examples = []\n",
        "  for data, target in test_loader:\n",
        "\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      data.requires_grad = True\n",
        "      \n",
        "      output = model(data)\n",
        "\n",
        "      init_pred = output.max(1, keepdim=True)[1]\n",
        "      \n",
        "      if init_pred.item() != target.item(): # if the initial prediction is wrong, do not bother attacking\n",
        "        continue\n",
        "        \n",
        "      loss = F.nll_loss(output, target)\n",
        "      \n",
        "      model.zero_grad()\n",
        "      \n",
        "      loss.backward()\n",
        "      \n",
        "      # collect datagrad\n",
        "      data_grad = data.grad.data\n",
        "      \n",
        "      # call FGSM attack\n",
        "      perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "      \n",
        "      # reclassify the perturbed image\n",
        "      output = model(perturbed_data)\n",
        "      \n",
        "      # check for success\n",
        "      final_pred = output.max(1, keepdim=True)[1] # get the index of the max log probability\n",
        "      if final_pred.item() == target.item():\n",
        "        correct += 1\n",
        "        if (epsilon == 0) and (len(adv_examples) < 5):\n",
        "          # special case for saving 0 epsilon examples\n",
        "          adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "          adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
        "        else:\n",
        "          # save some adv examples for visualization later\n",
        "          if len(adv_examples) < 5:\n",
        "            adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "            adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
        "  \n",
        "  \n",
        "  # calculate final accuracy for this epsilon\n",
        "  final_acc = correct/float(len(test_loader))\n",
        "\n",
        "  print('Epsilon: {}\\t, Test Accuracy = {} / {} = {}'.format(epsilon, correct,\n",
        "                                                                              len(test_loader),\n",
        "                                                                              final_acc))\n",
        "  # return the accuracy and an adversarial example\n",
        "  return final_acc, adv_examples\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APt8nVpJ9ifZ",
        "colab_type": "text"
      },
      "source": [
        "# show image with landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayEVnAEX9lL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show image with landmarks\n",
        "def show_landmarks(image, landmarks):\n",
        "  plt.imshow(image)\n",
        "  plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
        "  plt.pause(0.001) # pause a bit so that plots are updated\n",
        "\n",
        "plt.figure()\n",
        "show_landmarks(io.imread(os.path.join('faces/', img_name)), landmarks)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnGiQPDHFFci",
        "colab_type": "text"
      },
      "source": [
        "# CustomTransforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAXEjB2mFISv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rescale(object):\n",
        "  \"\"\"\n",
        "  \n",
        "  Rescale the image in a sample to a given size.\n",
        "  \n",
        "  Args:\n",
        "    output_size (tuple or int): desired output size.\n",
        "    if tuple, output is matched to output_size.\n",
        "    if int, smaller of image edges is matched to output_size keeping aspect ratio the same.\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    self.output_size = output_size\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    \n",
        "    h, w = image.shape[:2]\n",
        "    \n",
        "    if isinstance(self.output_size, int):\n",
        "      if h > w:\n",
        "        new_h, new_w = self.output_size * h / w, self.output_size\n",
        "      else:\n",
        "        new_h, new_w = self.output_size, self.output_size * w / h\n",
        "    else:\n",
        "      new_h, new_w = int(new_h), int(new_w)\n",
        "      \n",
        "    img = transform.resize(image, (new_h, new_w))\n",
        "    \n",
        "    # h and w are swapped for landmarks because for images, x and y axes are axis 1 and 0 respectively\n",
        "    \n",
        "    landmarks = landmarks * [new_w / w, new_h / h]\n",
        "    \n",
        "    return {'image': img, 'landmarks': landmarks}\n",
        "\n",
        "class RandomCrop(object):\n",
        "  \"\"\"\n",
        "  \n",
        "  Crop randomly the image in a sample.\n",
        "  \n",
        "  Args:\n",
        "    output_size (tuple or int): desired output size.\n",
        "    if int, square crop is made.\n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    if isinstance(output_size, int):\n",
        "      self.output_size = (output_size, output_size)\n",
        "    else:\n",
        "      assert len(output_size) == 2\n",
        "      self.output_size = output_size\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    h, w = iamge.shape[:2]\n",
        "    new_h, new_w = self.output_size\n",
        "    \n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "    \n",
        "    image = image[top: top + new_h, left: left + new_w]\n",
        "    \n",
        "    landmarks = landmarks - [left, top]\n",
        "    \n",
        "    return {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "class ToTensor(object):\n",
        "  \"\"\"\n",
        "  \n",
        "  convert ndarrays in sample to tensors.\n",
        "  \n",
        "  \"\"\"\n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    \n",
        "    # swap color axis because\n",
        "    # numpy image : H x W x C\n",
        "    # torch image : C x H x w\n",
        "    \n",
        "    image = image.transpose((2, 0, 1))\n",
        "    \n",
        "    return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)}\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvK-QV83Xblh",
        "colab_type": "text"
      },
      "source": [
        "# sparsity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjQvd6F1Xc2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparsity(cl_data_file):\n",
        "  class_list = cl_data_file.keys()\n",
        "  cl_sparsity = []\n",
        "  for cl in class_list:\n",
        "    cl_sparsity.append(np.mean([np.sum(x!=0) for x in cl_data_file[cl]]))\n",
        "    \n",
        "  return np.mean(cl_sparsity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_5-3qtXge_",
        "colab_type": "text"
      },
      "source": [
        "# one_hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-bz0c9sXiTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(y, num_class):\n",
        "  return torch.zeros((len(y), num_class)).scatter_(1, y.unsqueeze(1), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwxXXuDCXlLN",
        "colab_type": "text"
      },
      "source": [
        "# DBindex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBXyx6NuXmyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DBindex(cl_data_file):\n",
        "  class_list = cl_data_file.keys()\n",
        "  cl_num = len(class_list)\n",
        "  cl_means = []\n",
        "  stds = []\n",
        "  DBs = []\n",
        "  for cl in class_list:\n",
        "    cl_means.append(np.mean(cl_data_file[cl], axis=0))\n",
        "    stds.append(np.sqrt(np.mean(np.sum(np.square(cl_data_file[cl] - cl_means[-1]), axis = 1))))\n",
        "    \n",
        "  mu_i = np.tile(np.expand_dims(np.array(cl_means), axis = 0), len(class_list), 1, 1)\n",
        "  mu_j = np.transpose(mu_i, (1, 0, 2))\n",
        "  mdists = np.sqrt(np.sum(np.square(mu_i - mu_j), axis = 2))\n",
        "  \n",
        "  for i in range(cl_num):\n",
        "    DBs.append(np.max([(stds[i] + stds[j])/mdists[i, j] for j in range(cl_num) if j != i]))\n",
        "  return np.mean(DBs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AUb1vFTaf0F",
        "colab_type": "text"
      },
      "source": [
        "# CustomTransforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDLROP4PahkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformLoader:\n",
        "  def __init__(self, image_size, normalize_param = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "               jitter_param=dict(Brightness=0.4, Contrast=0.4, Color=0.4)):\n",
        "    self.image_size = image_size\n",
        "    self.normalize_param = normalize_param\n",
        "    self.jitter_param = jitter_param\n",
        "    \n",
        "  def parse_transform(self, transform_type):\n",
        "    if transform_type == 'ImageJitter':\n",
        "      method = add_transforms.ImageJitter(self.jitter_param)\n",
        "      return method\n",
        "    method = getattr(transforms, transform_type)\n",
        "    if transform_type == 'RandomSizedCrop':\n",
        "      return method(self.image_size)\n",
        "    elif transform_type == 'CenterCrop':\n",
        "      return method(self.image_size)\n",
        "    elif transform_type == 'Scale':\n",
        "      return method([int(self.image_size*1.15), int(self.image_size*1.15)])\n",
        "    elif transform_type == 'Normalize':\n",
        "      return method(**self.normalize_param)\n",
        "    else:\n",
        "      return method()\n",
        "    \n",
        "  def get_composed_transform(self, aug = False):\n",
        "    if aug:\n",
        "      transform_list = ['RandomSizedCrop', 'ImageJitter', 'RandomHorizontalFlip', 'ToTensor', 'Normalize']\n",
        "    else:\n",
        "      transform_list = ['Scale', 'CenterCrop', 'ToTensor', 'Normalize']\n",
        "    \n",
        "    transform_funcs = [self.parse_transform(x) for x in transform_list]\n",
        "    transform = transforms.Compose(transform_funcs)\n",
        "    return transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2fOzmLHc_Wp",
        "colab_type": "text"
      },
      "source": [
        "# SimpleDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_PggqqRdAmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleDataset:\n",
        "  def __init__(self, data_file, transform, target_transform=identity):\n",
        "    with open(data_file, 'r') as f:\n",
        "      self.meta = json.load(f)\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    image_path = os.path.join(self.meta['image_names'][i])\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "    target = self.target_transform(self.meta['image_labels'])\n",
        "    return img, target\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.meta['image_names'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l-Ufa8aeq5M",
        "colab_type": "text"
      },
      "source": [
        "# SetDataset, SubDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-NO50Tlet7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SetDataset:\n",
        "  def __init__(self, data_file, batch_size, transform):\n",
        "    with open(data_file, 'r') as f:\n",
        "      self.meta = json.load(f)\n",
        "      \n",
        "    self.cl_list = np.unique(self.meta['image_labels']).tolist()\n",
        "    \n",
        "    self.sub_meta = {}\n",
        "    \n",
        "    for cl in self.cl_list:\n",
        "      self.sub_meta[cl] = []\n",
        "      \n",
        "    for x, y in zip(self.meta['image_names'], self.meta['image_labels']):\n",
        "      self.sub_meta[y].append(x)\n",
        "    \n",
        "    self.sub_dataloader = []\n",
        "    \n",
        "    # use main thread only or may receive multiple batches\n",
        "    sub_data_loader_params = dict(batch_size = batch_size, shuffle = True, num_workers = 0, pin_memory = False)\n",
        "    \n",
        "    for cl in self.cl_list:\n",
        "      sub_dataset = SubDataset(self.sub_meta[cl], cl, transform = transform)\n",
        "      self.sub_dataloader.append(DataLoader(sub_dataset, **sub_data_loader_params))\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    return next(iter(self.sub_dataloader[i]))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.cl_list)\n",
        "  \n",
        "class SubDataset:\n",
        "  def __init__(self, sub_meta, cl, transform=transforms.ToTensor(), target_transform=identity):\n",
        "    self.sub_meta = sub_meta\n",
        "    self.cl = cl\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    image_path = os.path.join(self.sub_meta[i])\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "    target = self.target_transform(self.cl)\n",
        "    return img, target\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.sub_meta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4jzJ8sifFqj",
        "colab_type": "text"
      },
      "source": [
        "# EpisodicBatchSampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m57wRwGfIAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpisodicBatchSampler(object):\n",
        "  def __init__(self, n_classes, n_way, n_episodes):\n",
        "    self.n_classes = n_classes\n",
        "    self.n_way = n_way\n",
        "    self.n_episodes = n_episodes\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.n_episodes\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for i in range(self.n_episodes):\n",
        "      yield torch.randperm(self.n_classes)[:self.n_way]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzovOWlXfydt",
        "colab_type": "text"
      },
      "source": [
        "# SimpleHDF5Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWnkwG5f0mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature loader\n",
        "\n",
        "class SimpleHDF5Dataset:\n",
        "  def __init__(self, file_handle = None):\n",
        "    if file_handle == None:\n",
        "      self.f = ''\n",
        "      self.all_feats_dset = []\n",
        "      self.all_labels = []\n",
        "      self.total = 0\n",
        "    else:\n",
        "      self.f = file_handle\n",
        "      self.all_feats_dset = self.f['all_feats'][...]\n",
        "      self.all_labels = self.f['all_labels'][...]\n",
        "      self.total = self.f['count'][0]\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    return torch.Tensor(self.all_feats_dset[i, :], int(self.all_labels[i]))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.total\n",
        "  \n",
        "def init_loader(filename):\n",
        "  with h5py.File(filename, 'r') as f:\n",
        "    fileset = SimpleHDF5Dataset(f)\n",
        "  \n",
        "  feats = fileset.all_feats_dset\n",
        "  labels = fileset.all_labels\n",
        "  while np.sum(feats[-1]) == 0:\n",
        "    feats = np.delete(feats, -1, axis = 0)\n",
        "    labels = np.delete(labels, -1, axis = 0)\n",
        "  \n",
        "  class_list = np.unique(np.array(labels)).tolist()\n",
        "  inds = range(len(labels))\n",
        "  \n",
        "  cl_data_file = {}\n",
        "  for cl in class_list:\n",
        "    cl_data_file[cl] = []\n",
        "  for ind in inds:\n",
        "    cl_data_file[labels[ind]].append(feats[ind])\n",
        "  \n",
        "  return cl_data_file\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TRMIFSR39Lv",
        "colab_type": "text"
      },
      "source": [
        "# multiple models save, load\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrNmfVqG3_V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save\n",
        "torch.save({\n",
        "    'modelA_state_dict': modelA.state_dict(),\n",
        "    'modelB_state_dict': modelB.state_dict(),\n",
        "    'optimizerA_state_dict': optimizerA.state_dict(),\n",
        "    'optimizerB_state_dict': optimizerB.state_dict(),\n",
        "    ...\n",
        "}, PATH)\n",
        "\n",
        "# load\n",
        "modelA = TheModelAClass(*args, **kwargs)\n",
        "modelB = TheModelBClass(*args, **kwargs)\n",
        "optimizerA = TheOptimizerAClass(*args, **kwargs)\n",
        "optimizerB = TheOptimizerBClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
        "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
        "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
        "\n",
        "modelA.eval()\n",
        "modelB.eval()\n",
        "# or\n",
        "modelA.train()\n",
        "modelB.train()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J8M1X5D4Ro3",
        "colab_type": "text"
      },
      "source": [
        "# saving and loading a general checkpoint for inference and/or resuming training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ehYMsu4TJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save:  \n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss,\n",
        "    ...\n",
        "\n",
        "}, PATH)\n",
        "\n",
        "# load:\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "optimizer = TheOptimizerClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "model.eval()\n",
        "# or\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeRFhvkI4eKn",
        "colab_type": "text"
      },
      "source": [
        "# save/load entire model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CjAdF8Y4f0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save\n",
        "torch.save(model, PATH)\n",
        "  \n",
        "# load\n",
        "# model class must be defined somewhere\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ryz8J04lhS",
        "colab_type": "text"
      },
      "source": [
        "# saving and loading model state_dict for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vxd22xB4oyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# load:\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7p9jakt54Ux",
        "colab_type": "text"
      },
      "source": [
        "# warmstarting model using parameters from a different model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67WgRCrx56hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save\n",
        "torch.save(modelA.state_dict(), PATH)\n",
        "  \n",
        "# load\n",
        "modelB = TheModelBClass(*args, **kwargs)\n",
        "modelB.load_state_dict(torch.load(PATH), strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbudFfEBNPV6",
        "colab_type": "text"
      },
      "source": [
        "# CNNEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ9Rdy9NNQvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNEncoder, self).__init__()\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=3, padding=0),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(2))\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=0),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(2))\n",
        "    self.layer3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU()\n",
        "                               )\n",
        "    self.layer4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU()\n",
        "                               )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYGVjmKBOZS7",
        "colab_type": "text"
      },
      "source": [
        "# RelationNetwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPsfhtiEOa9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RelationNetwork(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(RelationNetwork, self).__init__()\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(2))\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(2))\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(x)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.sigmoid(self.fc2(out))\n",
        "    return out\n",
        "\n",
        "def weights_init(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "    if m.bias is not None:\n",
        "      m.bias.data.zero_()\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "    m.weight.data.fill_(1)\n",
        "    m.bias.data.zero_()\n",
        "  elif classname.find('Linear') != -1:\n",
        "    n = m.weight.size(1)\n",
        "    m.weight.data.normal_(0, 0.01)\n",
        "    m.bias.data = torch.ones(m.bias.data.size())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMA1ea-efEc7",
        "colab_type": "text"
      },
      "source": [
        "# FullyContextualEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3_fJ0e4fIui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullyContextualEmbedding(nn.Module):\n",
        "  def __init__(self, feat_dim):\n",
        "    super(FullyContextualEmbedding, self).__init__()\n",
        "    self.lstmcell = nn.LSTMCell(feat_dim+2, feat_dim)\n",
        "    self.softmax = nn.Softmax()\n",
        "    self.c_0 = Variable(torch.zeros(1, feat_dim))\n",
        "    self.feat_dim = feat_dim\n",
        "    \n",
        "  def forward(self, f, G):\n",
        "    h = f\n",
        "    c = self.c_0.expand_as(f)\n",
        "    G_T = G.transpose(0, 1)\n",
        "    K = G.size(0)\n",
        "    for k in range(K):\n",
        "      logit_a = h.mm(G_T)\n",
        "      a = self.softmax(logit_a)\n",
        "      r = a.mm(G)\n",
        "      x = torch.cat((f, r), 1)\n",
        "      \n",
        "      h, c = self.lstmcell(x, (h, c))\n",
        "      h = h + f\n",
        "    return h\n",
        "  \n",
        "  def cuda(self):\n",
        "    super(FullyContextualEmbedding, self).cuda()\n",
        "    self.c_0 = self.c_0.cuda()\n",
        "    return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLb4aCtvPe_W",
        "colab_type": "text"
      },
      "source": [
        "# init seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnwA6sbyPghF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_seed(opt):\n",
        "  \"\"\"\n",
        "  Disable cudnn to maximize reproducibility\n",
        "  \"\"\"\n",
        "  torch.cuda.cudnn_enabled = False\n",
        "  np.random.seed(opt.manual_seed)\n",
        "  torch.manual_seed(opt.manual_seed)\n",
        "  torch.cuda.manual_seed(opt.manual_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lF4okkW4oeI",
        "colab_type": "text"
      },
      "source": [
        "# show sixteen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJidVTOy4r38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_sixteen(images, titles=0):\n",
        "    f, axarr = plt.subplots(4, 4, figsize=(15, 15), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
        "    for idx, ax in enumerate(f.axes):\n",
        "        ax.imshow(images[idx])\n",
        "        ax.axis(\"off\")\n",
        "        if titles: ax.set_title(titles[idx])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fovBEoJX4wpE",
        "colab_type": "text"
      },
      "source": [
        "# show enhanced and original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWHnMf2R4zs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_enhanced_and_original(enhanced_img, titles=0):\n",
        "    f, axarr = plt.subplots(1, 2, figsize=(10, 6))\n",
        "    axarr[0].imshow(img)\n",
        "    axarr[0].axis(\"off\")\n",
        "    axarr[0].set_title(titles[0])\n",
        "    axarr[1].imshow(enhanced_img)\n",
        "    axarr[1].axis(\"off\")\n",
        "    axarr[1].set_title(titles[1])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqCLNydU45A3",
        "colab_type": "text"
      },
      "source": [
        "# show three magnitudes and original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWFnUfKV4846",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_three_magnitutes_and_original(images, titles=0):\n",
        "    f, axarr = plt.subplots(1, 4, figsize=(20, 10))\n",
        "    for idx, ax in enumerate(axarr):\n",
        "        if idx==0: ax.imshow(img)\n",
        "        else: ax.imshow(images[idx-1])\n",
        "        ax.axis(\"off\")\n",
        "        if titles: ax.set_title(titles[idx])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPTa7Ugq5Eys",
        "colab_type": "text"
      },
      "source": [
        "# rotate with fill"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7JmdfQK5GPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_with_fill(img, magnitude):\n",
        "    im2 = img.convert(\"RGBA\")\n",
        "    rot = im2.rotate(magnitude)\n",
        "    fff = Image.new(\"RGBA\", rot.size, (128,) * 4)\n",
        "    out = Image.composite(rot, fff, rot)\n",
        "    return out.convert(img.mode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxtjy5Xd5X2A",
        "colab_type": "text"
      },
      "source": [
        "# auto augment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSVgmGCp5aJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class ImageNetPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n",
        "        Example:\n",
        "        >>> policy = ImageNetPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     ImageNetPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n",
        "            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n",
        "            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment ImageNet Policy\"\n",
        "\n",
        "\n",
        "class CIFAR10Policy(object):\n",
        "    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n",
        "        Example:\n",
        "        >>> policy = CIFAR10Policy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     CIFAR10Policy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n",
        "            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n",
        "            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n",
        "            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n",
        "            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n",
        "            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n",
        "            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n",
        "            SubPolicy(0.2, \"equalize\", 8, 0.8, \"equalize\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n",
        "            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment CIFAR10 Policy\"\n",
        "\n",
        "\n",
        "class SVHNPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 25 Sub-policies on SVHN.\n",
        "        Example:\n",
        "        >>> policy = SVHNPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     SVHNPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.9, \"shearX\", 4, 0.2, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"shearY\", 8, 0.7, \"invert\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.6, \"solarize\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"invert\", 3, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 1, 0.9, \"rotate\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.9, \"shearX\", 4, 0.8, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"shearY\", 8, 0.4, \"invert\", 5, fillcolor),\n",
        "            SubPolicy(0.9, \"shearY\", 5, 0.2, \"solarize\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"invert\", 6, 0.8, \"autocontrast\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 3, 0.9, \"rotate\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.9, \"shearX\", 4, 0.3, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"shearY\", 8, 0.7, \"invert\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"equalize\", 5, 0.6, \"translateY\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"invert\", 4, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"contrast\", 3, 0.8, \"rotate\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"invert\", 5, 0.0, \"translateY\", 2, fillcolor),\n",
        "            SubPolicy(0.7, \"shearY\", 6, 0.4, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 0.8, \"rotate\", 4, fillcolor),\n",
        "            SubPolicy(0.3, \"shearY\", 7, 0.9, \"translateX\", 3, fillcolor),\n",
        "            SubPolicy(0.1, \"shearX\", 6, 0.6, \"invert\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"solarize\", 2, 0.6, \"translateY\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"shearY\", 4, 0.8, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.7, \"shearX\", 9, 0.8, \"translateY\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"shearY\", 5, 0.7, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"shearX\", 2, 0.1, \"invert\", 5, fillcolor)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment SVHN Policy\"\n",
        "\n",
        "\n",
        "class SubPolicy(object):\n",
        "    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n",
        "        ranges = {\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 150 / 331, 10),\n",
        "            \"translateY\": np.linspace(0, 150 / 331, 10),\n",
        "            \"rotate\": np.linspace(0, 30, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 0, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.9, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,\n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "\n",
        "        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n",
        "        def rotate_with_fill(img, magnitude):\n",
        "            rot = img.convert(\"RGBA\").rotate(magnitude)\n",
        "            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n",
        "\n",
        "        func = {\n",
        "            \"shearX\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "                Image.BICUBIC, fillcolor=fillcolor),\n",
        "            \"shearY\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "                Image.BICUBIC, fillcolor=fillcolor),\n",
        "            \"translateX\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "                fillcolor=fillcolor),\n",
        "            \"translateY\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n",
        "                fillcolor=fillcolor),\n",
        "            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n",
        "            # \"rotate\": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n",
        "            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n",
        "            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n",
        "            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n",
        "            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n",
        "            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n",
        "            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n",
        "        }\n",
        "\n",
        "        # self.name = \"{}_{:.2f}_and_{}_{:.2f}\".format(\n",
        "        #     operation1, ranges[operation1][magnitude_idx1],\n",
        "        #     operation2, ranges[operation2][magnitude_idx2])\n",
        "        self.p1 = p1\n",
        "        self.operation1 = func[operation1]\n",
        "        self.magnitude1 = ranges[operation1][magnitude_idx1]\n",
        "        self.p2 = p2\n",
        "        self.operation2 = func[operation2]\n",
        "        self.magnitude2 = ranges[operation2][magnitude_idx2]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n",
        "        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sphccv_KKZ89",
        "colab_type": "text"
      },
      "source": [
        "# tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwAmofm9KbpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with test_summary_writer.as_default():\n",
        "          summary.scalar('loss', test_loss, step=self.globaliter)\n",
        "          summary.scalar('accuracy', accuracy, step=self.globaliter)\n",
        "\n",
        "with train_summary_writer.as_default():\n",
        "          tf.summary.scalar('loss', loss.item(), step=globaliter)\n",
        "\n",
        "@tf.function\n",
        "def my_func(step, loss):\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar(\"loss\", loss.item(), step)\n",
        "    \n",
        "# call this function during training\n",
        "my_func(globaliter, loss)\n",
        "\n",
        "    \n",
        "%tensorboard --logdir logs/tensorboard\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dYUs9pwy96L",
        "colab_type": "text"
      },
      "source": [
        "# check runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR13GeLmy_ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RXoucIy4msV",
        "colab_type": "text"
      },
      "source": [
        "# weights_init_normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRMdFXtz4oub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init_normal(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find(\"Conv\") != -1:\n",
        "    torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "  elif classname.find(\"BatchNorm2d\") != -1:\n",
        "    torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    torch.nn.init.constant_(m.bias.data, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVxLEFhX5Wj9",
        "colab_type": "text"
      },
      "source": [
        "# ResidualBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZQErX4O5YtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, in_features):\n",
        "  # 2\n",
        "  def __init__(self, in_features, norm=\"in\"):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    \n",
        "    conv_block = [\n",
        "        nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n",
        "        nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n",
        "        nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True)\n",
        "    ]\n",
        "    \n",
        "    # 2\n",
        "    norm_layer = AdaptiveInstanceNorm2d if norm == \"adain\" else nn.InstanceNorm2d\n",
        "    \n",
        "    conv_block = [\n",
        "        nn.ReflectionPad2d(1),\n",
        "        nn.Conv2d(in_features, in_features, 3),\n",
        "        norm_layer(in_features),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.ReflectionPad2d(1),\n",
        "        nn.Conv2d(in_features, in_features, 3),\n",
        "        norm_layer(in_features),        \n",
        "    ]\n",
        "    \n",
        "    self.conv_block = nn.Sequential(*conv_block)\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    return x + self.conv_block(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYLtkimU7CDM",
        "colab_type": "text"
      },
      "source": [
        "# GeneratorResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaPSgvMB7EB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorResNet(nn.Module):\n",
        "  def __init__(self, img_shape=(3, 128, 128), res_blocks=9, c_dim=5):\n",
        "    super(GeneratorResNet, self).__init__()\n",
        "    channels, img_size, _ = img_shape\n",
        "    \n",
        "    # initial convolutional block\n",
        "    model = [\n",
        "        nn.Conv2d(channels + c_dim, 64, 7, stride=1, padding=3, bias=False),\n",
        "        nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(inplace=True)\n",
        "    ]\n",
        "    \n",
        "    # downsampling\n",
        "    curr_dim = 64\n",
        "    for _ in range(2):\n",
        "      model += [\n",
        "          nn.Conv2d(curr_dim, curr_dim * 2, 4, stride=2, padding=1, bias=False),\n",
        "          nn.InstanceNorm2d(curr_dim=2, affine=True, track_running_stats=True),\n",
        "          nn.ReLU(inplace=True)\n",
        "      ]\n",
        "      curr_dim *= 2\n",
        "    \n",
        "    # residual blocks\n",
        "    for _ in range(res_blocks):\n",
        "      model += [ResidualBlock(curr_dim)]\n",
        "    \n",
        "    # upsampling\n",
        "    for _ in range(2):\n",
        "      mode += [\n",
        "          nn.ConvTranspose2d(curr_dim, curr_dim // 2, 4, stride=2, padding=1, bias=False),\n",
        "          nn.InstanceNorm2d(curr_dim // 2, affine=True, track_running_stats=True),\n",
        "          nn.ReLU(inplace=True)\n",
        "      ]\n",
        "      curr_dim = curr_dim // 2\n",
        "    \n",
        "    # output layer\n",
        "    model += [nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3), nn.Tanh()]\n",
        "    \n",
        "    self.model = nn.Sequential(*model)\n",
        "  \n",
        "  def forward(self, x, c):\n",
        "    c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "    c = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "    x = torch.cat((x, c), 1)\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAdexZ6Q8Sb2",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XXaQs_q8UPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, img_shape=(3, 128, 128), c_dim=5, n_strided=6):\n",
        "  # 2\n",
        "  def __init__(self, input_shape):\n",
        "  # 3\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    # 1\n",
        "    channels, img_size, _ = img_shape\n",
        "    \n",
        "    # 2\n",
        "    channels, height, width = input_shape\n",
        "    \n",
        "    # calculate output of image discriminator (PatchGAN)\n",
        "    # 1\n",
        "    self.output_shape = (1, height // 2 ** 3, width // 2 ** 3)\n",
        "    # 2\n",
        "    self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
        "    \n",
        "    # 1\n",
        "    def discriminator_block(in_filters, out_filters):\n",
        "      \"\"\"\n",
        "      Returns downsampling layers of each discriminator block\n",
        "      \"\"\"\n",
        "      layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n",
        "      return layers\n",
        "    \n",
        "    # 2\n",
        "    def discriminator_block(in_filters, out_filters, normalization):\n",
        "      \"\"\"\n",
        "      Returns downsampling layers of each discriminator block\n",
        "      \"\"\"\n",
        "      # 1\n",
        "      layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "      # 2\n",
        "      layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True),\n",
        "               nn.Dropout2d(0.25)]\n",
        "      if normalization:\n",
        "        # 1\n",
        "        layers.append(nn.InstanceNorm2d(out_filters))\n",
        "        # 2\n",
        "        layers.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      return layers\n",
        "    # 3\n",
        "    def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "      layers = []\n",
        "      layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
        "      if not first_block:\n",
        "        layers.append(nn.BatchNorm2d(out_filters))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
        "      layers.append(nn.BatchNorm2d(out_filters))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      return layers\n",
        "    \n",
        "    \n",
        "    \n",
        "    # 1\n",
        "    layers = discriminator_block(channels, 64)\n",
        "    curr_dim = 64\n",
        "    for _ in range(n_strided - 1):\n",
        "      layers.extend(discriminator_block(curr_dim, curr_dim * 2))\n",
        "      curr_dim *= 2\n",
        "    \n",
        "    self.model = nn.Sequentail(*layers)\n",
        "    \n",
        "    # output 1: PatchGAN\n",
        "    self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n",
        "    \n",
        "    # output 2: class prediction\n",
        "    kernel_size = img_size // 2 ** n_strided\n",
        "    self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)\n",
        "  \n",
        "    # 2\n",
        "    self.model = nn.Sequential(\n",
        "        *discriminator_block(channels, 64, normalization=False),\n",
        "        *discriminator_block(64, 128),\n",
        "        *discriminator_block(128, 128),\n",
        "        nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "        # 1\n",
        "        nn.Conv2d(256, 1, 4, padding=1)\n",
        "        # 2\n",
        "        nn.Conv2d(256, 1, 4)\n",
        "    )\n",
        "    \n",
        "    # 3\n",
        "    self.model = nn.Sequential(\n",
        "        # 1\n",
        "        nn.Linear(opt.img_size ** 2, 512),\n",
        "        # 2\n",
        "        nn.Linear(int(np.prod(img_shape)), 512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(256, 1),\n",
        "        # 1/2\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "  \n",
        "    # 4\n",
        "    \n",
        "    layers = []\n",
        "    in_filters = in_channels\n",
        "    for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "      layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "      in_filters = out_filters\n",
        "    \n",
        "    layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    # 5\n",
        "    self.model = nn.Sequential(\n",
        "        *discriminator_block(channels, 16, normalization=False),\n",
        "        *discriminator_block(16, 32),\n",
        "        *discriminator_block(32, 64),\n",
        "        *discriminator_block(64, 128),\n",
        "    )\n",
        "    \n",
        "    # the height and width of downsampled image\n",
        "    ds_size = img_size // 2 ** 4\n",
        "    self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
        "    \n",
        "    # 6\n",
        "    # upsampling\n",
        "    self.down = nn.Sequential(nn.Conv2d(channels, 64, 3, 2, 1), nn.ReLU())\n",
        "    # fully connected layers\n",
        "    self.down_size = img_size // 2\n",
        "    down_dim = 64 * (img_size // 2) ** 2\n",
        "    \n",
        "    self.embedding = nn.Linear(down_dim, 32)\n",
        "    \n",
        "    self.fc = nn.Sequential(\n",
        "        nn.BatchNorm1d(32, 0.8),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(32, down_dim),\n",
        "        nn.BatchNorm1d(down_dim),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "    \n",
        "    # upsampling\n",
        "    self.up = nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(64, channels, 3, 1, 1))\n",
        "  \n",
        "  # 1\n",
        "  def forward(self, img):\n",
        "    feature_repr = self.model(img)\n",
        "    out_adv = self.out1(feature_repr)\n",
        "    out_cls = self.out2(feature_repr)\n",
        "    return out_adv, out_cls.view(out_cls.size(0), -1)\n",
        "  \n",
        "  # 2\n",
        "  def forward(self, img):\n",
        "    # concantenate image and condition image by channels to produce input\n",
        "    return self.model(img)\n",
        "  \n",
        "  # 3\n",
        "  def forward(self, img):\n",
        "    img_flat = img.view(img.shape[0], -1)\n",
        "    validity = self.model(img_flat)\n",
        "    return validity\n",
        "  \n",
        "  # 4\n",
        "  def forward(self, img):\n",
        "    out = self.model(img)\n",
        "    out = out.view(out.shape[0], -1)\n",
        "    validity = self.adv_layer(out)\n",
        "    return validity\n",
        "  \n",
        "  # 5\n",
        "  def forward(self, img):\n",
        "    out = self.down(img)\n",
        "    embedding = self.embedding(out.view(out.size(0), -1))\n",
        "    out = self.fc(embedding)\n",
        "    out = self.up(out.view(out.size(0), 64, self.down_size, self.down_size))\n",
        "    return out, embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYBel5CXCUXy",
        "colab_type": "text"
      },
      "source": [
        "# FeatureExtractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCy3zz-xCWV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    vgg19_model = models.vgg19(pretrained=True)\n",
        "    self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
        "  \n",
        "  def forward(self, img):\n",
        "    return self.feature_extractor(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxPkHqv7XQgQ",
        "colab_type": "text"
      },
      "source": [
        "# UNetDown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKe1lYfLXSUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# U-NET\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "  def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "    super(UNetDown, self).__init__()\n",
        "    # 1\n",
        "    layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n",
        "    # 2\n",
        "    layers = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n",
        "    if normalize:\n",
        "      # 1\n",
        "      layers.append(nn.BatchNorm2d(out_size, 0.8))\n",
        "      # 2\n",
        "      layers.append(nn.InstanceNorm2d(out_size))\n",
        "      # 3\n",
        "      layers.append(nn.InstanceNorm2d(out_size, affine=True))\n",
        "    if dropout:\n",
        "      layers.append(nn.Dropout(dropout))\n",
        "    layers.append(nn.LeakyReLU(0.2))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiyWZtUZXTuo",
        "colab_type": "text"
      },
      "source": [
        "# UNetUp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yliKqdwXYyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNetUp(nn.Module):\n",
        "  def __init__(self, in_size, out_size):\n",
        "    super(UNetUp, self).__init__()\n",
        "    # 1\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_size, 0.8),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "    \n",
        "    # 2\n",
        "    layers = [# 1\n",
        "              nn.ConvTranspose2d(in_size, out_size, 4, 2, 1), \n",
        "              # 2\n",
        "              nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False), \n",
        "              # 1\n",
        "              nn.InstanceNorm2d(out_size), \n",
        "              # 2\n",
        "              nn.BatchNorm2d(out_size, 0.8),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if dropout:\n",
        "      layers.append(nn.Dropout(dropout))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "  def forward(self, x, skip_input):\n",
        "    x = self.model(x)\n",
        "    x = torch.cat((x, skip_input), 1)\n",
        "    return x\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3JM77fQakca",
        "colab_type": "text"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO7TlbaqamRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, latent_dim, input_shape):\n",
        "    super(Encoder, self).__init__()\n",
        "  \n",
        "    resnet18_model = resnet18(pretrained=False)\n",
        "    self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n",
        "    self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
        "    # output is mu and log(var) for reparameterization rick used in VAEs\n",
        "    self.fc_mu = nn.Linear(256, latent_dim)\n",
        "    self.fc_logvar = nn.Linear(256, latent_dim)\n",
        "    \n",
        "  def forward(self, img):\n",
        "    out = self.feature_extractor(img)\n",
        "    out = self.pooling(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    mu = self.fc_mu(out)\n",
        "    logvar = self.fc_logvar(out)\n",
        "    return mu, logvar\n",
        "\n",
        "# 2\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2, style_dim=8):\n",
        "    super(Encoder, self).__init__()\n",
        "  \n",
        "    self.content_encoder = ContentEncoder(in_channels, dim, n_residual, n_downsample)\n",
        "    self.style_encoder = StyleEncoder(in_channels, dim, n_downsample, style_dim)\n",
        "    \n",
        "  def forward(self, img):\n",
        "    content_code = self.content_encoder(x)\n",
        "    style_code = self.style_encoder(x)\n",
        "    return content_code, style_code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idCkpHgwehUt",
        "colab_type": "text"
      },
      "source": [
        "# GeneratorUNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKDJAWy4eigL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorUNet(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, latent_dim, img_shape):\n",
        "  # 2\n",
        "  def __init__(self, channels=3):\n",
        "    super(GeneratorUNet, self).__init__()\n",
        "    channels, self.h, self.w = img_shape\n",
        "    \n",
        "    # 1\n",
        "    self.fc = nn.Linear(latent_dim, self.h * self.w)\n",
        "    \n",
        "    # 1\n",
        "    self.down1 = UNetDown(channels + 1, 64, normalize=False)\n",
        "    # 2\n",
        "    self.down1 = UNetDown(channels, 64, normalize=False)\n",
        "    self.down2 = UNetDown(64, 128)\n",
        "    # 1\n",
        "    self.down3 = UNetDown(128, 256)\n",
        "    # 2\n",
        "    self.down3 = UNetDown(128 + channels, 256, dropout=0.5)\n",
        "    # 3\n",
        "    self.down3 = UNetDown(128, 256, dropout=0.5)\n",
        "    self.down4 = UNetDown(256, 512)\n",
        "    self.down5 = UNetDown(512, 512)\n",
        "    self.down6 = UNetDown(512, 512)\n",
        "    self.down7 = UNetDown(512, 512, normalize=False)\n",
        "    self.up1 = UNetUp(512, 512)\n",
        "    self.up2 = UNetUp(1024, 512)\n",
        "    self.up3 = UNetUp(1024, 512)\n",
        "    self.up4 = UNetUp(1024, 256)\n",
        "    self.up5 = UNetUp(512, 128)\n",
        "    self.up6 = UNetUp(256, 64)\n",
        "  \n",
        "    # 1\n",
        "    self.final = nn.Sequential(\n",
        "        nn.Upsample(scale_factor),\n",
        "        nn.Conv2d(128, channels, 3, stride=1, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    \n",
        "    # 2\n",
        "    channels, _, _ = input_shape\n",
        "    \n",
        "    self.down1 = UNetDown(channels, 64, normalize=False)\n",
        "    self.down2 = UNetDown(64, 128)\n",
        "    self.down3 = UNetDown(128, 256, dropout=0.5)\n",
        "    self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "    self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "    # 1\n",
        "    self.down6 = UNetDown(512, 512, dropout=0.5, normalize=False)\n",
        "    # 2\n",
        "    self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "    \n",
        "    self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "    self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "    self.up3 = UNetUp(1024, 256)\n",
        "    self.up4 = UNetUp(512, 128)\n",
        "    self.up5 = UNetUp(256, 64)\n",
        "    \n",
        "    # 2\n",
        "    self.final = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "        nn.Conv2d(128, channels, 4, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  \n",
        "    # 3\n",
        "    self.final = nn.Sequential(\n",
        "        nn.ConvTranspose2d(128, channels, 4, stride=2, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  \n",
        "  \n",
        "  def forward(self, x, c):\n",
        "    # 1\n",
        "    # propagate noise through fc layer and reshape to img shape\n",
        "    z = self.fc(z).view(z.size(0), 1, self.h, self.w)\n",
        "    d1 = self.down1(torch.cat((x, z), 1))\n",
        "    d2 = self.down2(d1)\n",
        "    d3 = self.down2(d2)\n",
        "    d4 = self.down2(d3)\n",
        "    d5 = self.down2(d4)\n",
        "    d6 = self.down2(d5)\n",
        "    d7 = self.down2(d6)\n",
        "    u1 = self.up1(d7, d6)\n",
        "    u2 = self.up2(u1, d5)\n",
        "    u3 = self.up3(u2, u4)\n",
        "    u4 = self.up4(u3, d3)\n",
        "    u5 = self.up5(u4, d2)\n",
        "    u6 = self.up6(u5, d1)\n",
        "    return self.final(u6)\n",
        "  \n",
        "    # 2\n",
        "    # U-Net generator with skip connections from encoder to decoder\n",
        "    d1 = self.down1(x)\n",
        "    d2 = self.down2(d1)\n",
        "    d2 = torch.cat((d2, x_lr), 1)\n",
        "    d3 = self.down2(d2)\n",
        "    d4 = self.down2(d3)\n",
        "    d5 = self.down2(d4)\n",
        "    d6 = self.down2(d5)\n",
        "    u1 = self.up1(d6, d5)\n",
        "    u2 = self.up2(u1, d4)\n",
        "    u3 = self.up3(u2, d3)\n",
        "    u4 = self.up4(u3, d2)\n",
        "    u5 = self.up5(u4, d1)\n",
        "    return self.final(u5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IpmmdbVocon",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpeAqZbCod4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    # 1\n",
        "    def block(in_feat, out_feat, normalize=True):\n",
        "      layers = [nn.Linear(in_feat, out_feat)]\n",
        "      if normalize:\n",
        "        layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      return layers\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "        *block(latent_dim, 128, normalize=False),\n",
        "        *block(128, 256),\n",
        "        *block(256, 512),\n",
        "        *block(512, 1024),\n",
        "        nn.Linear(1024, int(np.prod(img_shape))),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    \n",
        "    # 2\n",
        "    self.init_size = img_size // 4\n",
        "    self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
        "    \n",
        "    self.conv_blocks = nn.Sequential(\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(128, 0.8),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64, 0.8),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
        "        nn.Tanh(),\n",
        "    )\n",
        "  # 1  \n",
        "  def forward(self, z):\n",
        "    img = self.model(z)\n",
        "    img = img.view(img.shape[0], *img_shape)\n",
        "    return img\n",
        "  # 2\n",
        "  def forward(self, noise):\n",
        "    out = self.l1(noise)\n",
        "    out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "    img = self.conv_blocks(out)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_eMay8z9psP",
        "colab_type": "text"
      },
      "source": [
        "# GAN optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVVIiyse9rDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer, example, Adadelta, Adagrad, SparseAdam, Adamax, ASGD, LBFGS, RMSprop, Rprop, SGD\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(b1, b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6H5GGu_TdC",
        "colab_type": "text"
      },
      "source": [
        "# gradient penalty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c50eBQI1_WcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_gradient_penalty(D, X):\n",
        "  \"\"\"\n",
        "  calculates the gradient penalty loss for DRAGAN\n",
        "  \"\"\"\n",
        "  # random weight term for interpolation\n",
        "  alpha = Tensor(np.random.random(size=X.shape))\n",
        "  \n",
        "  interpolates = alpha * X + ((1 - alpha) * (X + 0.5 * X.std() * torch.rand(X.size())))\n",
        "  interpolates = Variable(interpoates, requires_grad=True)\n",
        "  \n",
        "  d_interpolates = D(interpolates)\n",
        "  fake = Variable(Tensor(X.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "  \n",
        "  # get gradient with respect to interpolates\n",
        "  gradients = autograd.grad(\n",
        "      outputs=d_interpolates,\n",
        "      inputs=interpolates,\n",
        "      grad_outputs=fake,\n",
        "      create_graph=True,\n",
        "      retain_graph=True,\n",
        "      only_inputs=True,\n",
        "  )[0]\n",
        "  \n",
        "  gradient_penalty = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "  return gradient_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSTirMZB1Fb4",
        "colab_type": "text"
      },
      "source": [
        "# initialize generator and discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqJ5Dith1JBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize generator and discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAAb_J6Z1QGK",
        "colab_type": "text"
      },
      "source": [
        "# GAN optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1I1MC261SOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_G = Adam(generator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "optimizer_D = Adam(discriminator.parameters(), lr=learning_rate, betas=(b1, b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKTnnyQG3RGG",
        "colab_type": "text"
      },
      "source": [
        "# GAN hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D18rbYk3TVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GAN hyperparameters\n",
        "\n",
        "num_epochs =  200\n",
        "batch_size = 64\n",
        "learning_rate = 0.0002\n",
        "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
        "b2 = 0.999 # adam: deccay of first order momentum of gradient\n",
        "latent_dim = 100 # dimensionality of the latent space\n",
        "img_size = 28 # size of each image dimension\n",
        "channels = 1 # number of image channels\n",
        "sample_interval = 400 # interval between image samples\n",
        "img_shape = (channels, img_size, img_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRFK1vLcFgbB",
        "colab_type": "text"
      },
      "source": [
        "# boundary seeking loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts23pTIXFjY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def boundary_seeking_loss(y_pred, y_true):\n",
        "  return 0.5 * torch.mean((torch.log(y_pred) - torch.log(1 - y_pred)) ** 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCejXsNQKT1O",
        "colab_type": "text"
      },
      "source": [
        "# GAN train generator, discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ3OV7MGKfYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for i, (imgs, _) in enumerate(mnist_loader):\n",
        "    \n",
        "    # adversarial ground truths\n",
        "    valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "    fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
        "    \n",
        "    # configure input\n",
        "    real_imgs = Variable(imgs.type(Tensor))\n",
        "    \n",
        "    # TRAIN GENERATOR\n",
        "    \n",
        "    optimizer_G.zero_grad()\n",
        "    \n",
        "    # sample noise as generator input\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "    \n",
        "    # generate a batch of images\n",
        "    gen_imgs = generator(z)\n",
        "    \n",
        "    # 1\n",
        "    # loss measures generator's ability to fool the discriminator\n",
        "    g_loss = boundary_seeking_loss(discriminator(gen_imgs), valid)\n",
        "    \n",
        "    # 2\n",
        "    # loss measures generator's ability to fool the discriminator\n",
        "    g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "    \n",
        "    # 3\n",
        "    # loss measures generator's ability to fool the discriminator\n",
        "    g_loss = pixelwise_loss(recon_images, gen_imgs.detach() + lambda_pt * pullaway_loss(img_embeddings))\n",
        "    \n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "    \n",
        "    # TRAIN DISCRIMINATOR\n",
        "    \n",
        "    optimizer_D.zero_grad()\n",
        "    \n",
        "    # 1\n",
        "    # measure discriminator's ability to classify real from generated samples\n",
        "    real_loss = discriminator_loss(discriminator(real_imgs), valid)\n",
        "    fake_loss = discriminator_loss(discriminator(gen_imgs.detach()), fake)\n",
        "    \n",
        "    # 2\n",
        "    # measure discriminator's ability to classify real from generated samples\n",
        "    real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "    fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "    \n",
        "    d_loss = (real_loss + fake_loss) / 2\n",
        "    \n",
        "    # 3\n",
        "    # measure discriminator's ability to classify real from generated samples\n",
        "    real_recon, _ = discriminator(real_imgs)\n",
        "    fake_recon, _ = discriminator(gen_imgs.detach())\n",
        "    \n",
        "    d_loss_real = pixelwise_loss(real_recon, real_imgs)\n",
        "    d_loss_fake = pixelwise_loss(fake_recon, gen_imgs.detach())\n",
        "    \n",
        "    d_loss = d_loss_real\n",
        "    if (margin - d_loss_fake.data).item() > 0:\n",
        "      d_loss += margin - d_loss_fake\n",
        "      \n",
        "    \n",
        "    # call this function during training\n",
        "    my_func(i, d_loss, g_loss)\n",
        "    \n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "    \n",
        "    print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"% (epoch, num_epochs, i, len(mnist_loader),\n",
        "                                                                    d_loss.item(), g_loss.item()\n",
        "                                                                   ))\n",
        "    \n",
        "    batches_done = epoch * len(mnist_loader) + i\n",
        "    if batches_done % sample_interval == 0:\n",
        "      save_image(gen_imgs.data[:25], \"./%d.png\" % batches_done, nrow=5, normalize=True)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17VepsDJCcvo",
        "colab_type": "text"
      },
      "source": [
        "# apply weight init normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs815ElmCfCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQli9V0sEC6z",
        "colab_type": "text"
      },
      "source": [
        "# GAN loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV38GACzEEp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reconstruction loss of AE\n",
        "pixelwise_loss = nn.MSELoss()\n",
        "\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# minimize MSE instead of BCE\n",
        "adversarial_loss = torch.nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnIaETSeJbMj",
        "colab_type": "text"
      },
      "source": [
        "# GAN tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGUz-gbLJcjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def my_func(step, d_loss, g_loss):\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar(\"d_loss\", d_loss.item(), step)\n",
        "    tf.summary.scalar(\"g_loss\", d_loss.item(), step)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE0iNEHALs8N",
        "colab_type": "text"
      },
      "source": [
        "# LambdaLR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbGmDSBZLunS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LambdaLR:\n",
        "  def __init__(self, num_epochs, offset, decay_start_epoch):\n",
        "    assert (num_epochs - decay_start_epoch) > 0, \"Decay must start befor the training session ends!\"\n",
        "    self.num_epochs = num_epocsh\n",
        "    self.offset = offset\n",
        "    self.decay_start_epoch = decay_start_epoch\n",
        "  \n",
        "  def step(self, epoch):\n",
        "    return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.num_epochs - \n",
        "                                                                         self.decay_start_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgWAhMX-OYZ7",
        "colab_type": "text"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVF7lqVcOZfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2, style_dim=8):\n",
        "    super(Decoder, self).__init__()\n",
        "    \n",
        "    layers = []\n",
        "    dim = dim * 2 ** n_upsample\n",
        "    \n",
        "    # residual blocks\n",
        "    for _ in range(n_residual):\n",
        "      layers += [ResidualBlock(dim, norm=\"adain\")]\n",
        "    \n",
        "    # upsampling\n",
        "    for _ in range(n_upsample):\n",
        "      layers += [\n",
        "          nn.Upsample(scale_factor=2),\n",
        "          nn.Conv2d(dim, dim // 2, 5, stride=1, padding=2),\n",
        "          LayerNorm(dim // 2),\n",
        "          nn.ReLU(inplace=True),\n",
        "      ]\n",
        "      dim = dim // 2\n",
        "    \n",
        "    # output layer\n",
        "    layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    # initiate mlp (predicts AdaIN parameters)\n",
        "    num_adain_params = self.get_num_adain_params()\n",
        "    self.mlp = MLP(style_dim, num_adain_params)\n",
        "  \n",
        "  def get_num_adain_params(self):\n",
        "    \"\"\"\n",
        "    return the number of AdaIN parameters needed by the model\n",
        "    \"\"\"\n",
        "    num_adain_params = 0\n",
        "    for m in self.modules():\n",
        "      if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
        "        num_adain_params += 2 * m.num_features\n",
        "    return num_adain_params\n",
        "  \n",
        "  def assign_adain_params(self, adain_params):\n",
        "    \"\"\"\n",
        "    assign the adain_params to the AdaIN layers in model\n",
        "    \"\"\"\n",
        "    for m in self.modules():\n",
        "      if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
        "        # extract mean and std predictions\n",
        "        mean = adain_params[:, : m.num_features]\n",
        "        std = adain_params[:, m.num_features : 2 * m.num_features]\n",
        "        # update bias and weight\n",
        "        m.bias = mean.contiguous().view(-1)\n",
        "        m.weight = std.contiguous().view(-1)\n",
        "        # move pointer\n",
        "        if adain_params.size(1) > 2 * m.num_features:\n",
        "          adain_params = adain_params[:, 2 * m.num_features :]\n",
        "  \n",
        "  def forward(self, content_code, style_code):\n",
        "    # update AdaIN parameters by MLP prediction based off style code\n",
        "    self.assign_adain_params(self.mlp(style_code))\n",
        "    img = self.model(content_code)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICR09H6OUk3i",
        "colab_type": "text"
      },
      "source": [
        "# ContentEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czH8HpS-Umr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContentEncoder(nn.Module):\n",
        "  def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2):\n",
        "    super(ContentEncoder, self).__init__()\n",
        "    \n",
        "    # initial convolution block\n",
        "    layers = [\n",
        "        nn.ReflectionPad2d(3),\n",
        "        nn.Conv2d(in_channels, dim, 7),\n",
        "        nn.InstanceNorm2d(dim),\n",
        "        nn.ReLU(inplace=True),\n",
        "    ]\n",
        "    \n",
        "    # downsampling\n",
        "    for _ in range(n_downsample):\n",
        "      layers += [\n",
        "          nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1),\n",
        "          nn.InstanceNorm2d(dim * 2),\n",
        "          nn.ReLU(inplace=True),\n",
        "      ]\n",
        "      dim *= 2\n",
        "    \n",
        "    # residual blocks\n",
        "    for _ in range(n_residual):\n",
        "      layers += [ResidualBlock(dim, norm=\"in\")]\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4QbQ4o0UnIN",
        "colab_type": "text"
      },
      "source": [
        "# StyleEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BDm3TRIUo0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleEncoder(nn.Module):\n",
        "  def __init__(self, in_channels=3, dim=64, n_downsample=2, style_dim=0):\n",
        "    super(StyleEncoder, self).__init__()\n",
        "    \n",
        "    # initial conv block\n",
        "    layers = [nn.ReflectionPad2d(3), nn.Conv2d(in_channels, dim, 7), nn.ReLU(inplace=True)]\n",
        "    \n",
        "    # downsampling\n",
        "    for _ in range(2):\n",
        "      layers += [nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n",
        "      \n",
        "    # downsampling with constant depth\n",
        "    for _ in range(n_downsample - 2):\n",
        "      layers += [nn.Conv2d(dim, dim, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n",
        "    \n",
        "    layers += [nn.AdaptiveAvgPool2d(1), nn.Conv2d(dim, style_dim, 1, 1, 0)]\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTuDmVOCV8Ge",
        "colab_type": "text"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Z47vtwV92x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MLP (predicts AdaIn parameters)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, dim=256, n_blk=3, activ=\"relu\"):\n",
        "    super(MLP, self).__init__()\n",
        "    layers = [nn.Linear(input_dim, dim), nn.ReLU(inplace=True)]\n",
        "    for _ in range(n_blk - 2):\n",
        "      layers += [nn.Linear(dim, dim), nn.ReLU(inplace=True)]\n",
        "    layers += [nn.Linear(dim, output_dim)]\n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.model(x.view(x.size(0), -1))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYvYtY2uZHkk",
        "colab_type": "text"
      },
      "source": [
        "# AdaptiveInstanceNorm2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUVN4rR8ZJWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveInstanceNorm2d(nn.Module):\n",
        "  def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "    super(AdaptiveInstanceNorm2d, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    # weight and bias are dynamically assigned\n",
        "    self.weight = None\n",
        "    self.bias = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    assert (\n",
        "      self.weight is not None and self.bias is not None\n",
        "    ), \"please assing weight and bias before calling AdaIN\"\n",
        "    b, c, h, w = x.size()\n",
        "    running_mean = self.running_mean.repeat(b)\n",
        "    running_var = self.running_var.repeat(b)\n",
        "    \n",
        "    # apply instance norm\n",
        "    x_reshaped = x.contiguous().view(1, b * c, h, w)\n",
        "    \n",
        "    out = F.batch_norm(\n",
        "        x_reshaped, running_mean, running_var, self.weight, self.bias, True, self.momentum, self.eps\n",
        "    )\n",
        "    \n",
        "    return out.view(b, c, h, w)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + \"(\" + str(self.num_features) + \")\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lOPQeM-aTTl",
        "colab_type": "text"
      },
      "source": [
        "# LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2iaeJhNaUSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, num_features, eps=1e-5, affine=True):\n",
        "    super(LayerNorm, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.affine = affine\n",
        "    self.eps = eps\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n",
        "      self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    shape = [-1] + [1] * (x.dim() - 1)\n",
        "    mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
        "    std = x.view(x.size(0), -1).std(1).view(*shape)\n",
        "    x = (x - mean) / (std + self.eps)\n",
        "    \n",
        "    if self.affine:\n",
        "      shape = [1, -1] + [1] * (x.dim() - 2)\n",
        "      x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}